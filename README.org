* 综述
  基于 Python 下的 Scrapy 框架，自动生成爬虫框架，实现只需添加初始 Url、抓取规则、
  处理抓取到的数据的方法等必须要做的事情，由 Scrapy 和这个脚本控制网页的并行抓取、
  出错处理、读取初始 Urls、爬虫目录的规范化等，便于快速开发爬虫.

  在这个基于 Scrapy 的自动生成的爬虫框架中提供了除 Scrapy 之外的这些功能:

  + 提供从 list 类型的变量、数据库、文件中读取初始 Url 的方法，只需在 list 类型
    的变量中添加初始 Urls，或者重写从数据库、文件读取初始 Url 的方法，就可以在爬
    虫代码所在的文件中自动读取这些 Url，使用者不用在爬虫代码中过多关心初始 Url
    的读取问题
  + 模拟浏览器行为，在发送的请求 Header 中添加了如 'Referer'、'User-Agent' 等内
    容，同时 'User-Agent' 是从提供的 444 个 user-agent 中随机提取的
  + 通过统一的配置文件管理 Scrapy 的参数配置、存储的数据库配置
  + 提供了几个工具函数，如处理抓取到的 url 的补全、获得满足数据库中时间的函数、
    去除抓取到的数据之间的空白之类等
* 使用方法
  在这个项目的当前目录下,执行如下操作
  
  $ sh autospider.sh 爬虫项目名 爬虫所在代码的文件名
  
  如
  
  $ sh autospider.sh testspider Crawler.py

  在新建立的爬虫中,如上述建立的爬虫中,先在 config.ini 文件中根据需求修改数据库和
  Scrapy 配置信息.然后在

  testspider/url/Urls.py

  文件中添加初始 urls.
  最后完善具体的爬虫的代码.
* *TODO*
  + 完善这个框架的功能，使用户把精力主要放在分析填写页面抓取规则、处理页面数据
  + 完善配置文件的管理
  + 添加日志模块监控页面的变化
